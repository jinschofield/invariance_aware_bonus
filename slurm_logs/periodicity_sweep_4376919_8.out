Sweep flags:  --use-int-norm

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2459
    train step  10000/20000 | loss=5.2435
    train step  15000/20000 | loss=5.2516
    train step  20000/20000 | loss=5.2418
    train step   5000/20000 | loss=1.3858
    train step  10000/20000 | loss=1.3818
    train step  15000/20000 | loss=1.3809
    train step  20000/20000 | loss=1.3804

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.3298
    train step  10000/20000 | loss=5.3328
    train step  15000/20000 | loss=5.3279
    train step  20000/20000 | loss=5.3004
    train step   5000/20000 | loss=1.0100
    train step  10000/20000 | loss=0.8138
    train step  15000/20000 | loss=0.6901
    train step  20000/20000 | loss=0.5286

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2202
    train step  10000/20000 | loss=5.2304
    train step  15000/20000 | loss=5.2118
    train step  20000/20000 | loss=5.2313
    train step   5000/20000 | loss=1.3637
    train step  10000/20000 | loss=1.2204
    train step  15000/20000 | loss=1.1756
    train step  20000/20000 | loss=1.2035

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2217
    train step  10000/20000 | loss=5.2294
    train step  15000/20000 | loss=5.2386
    train step  20000/20000 | loss=5.2191
    train step   5000/20000 | loss=1.3843
    train step  10000/20000 | loss=1.3840
    train step  15000/20000 | loss=1.3864
    train step  20000/20000 | loss=1.3801

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2581
    train step  10000/20000 | loss=5.2298
    train step  15000/20000 | loss=5.2264
    train step  20000/20000 | loss=5.2434
    train step   5000/20000 | loss=1.3137
    train step  10000/20000 | loss=1.2304
    train step  15000/20000 | loss=1.1515
    train step  20000/20000 | loss=1.1273

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
Study complete. Outputs in: /home/js6726/invariance_aware/periodicity_study/outputs_sweep/comb_8
