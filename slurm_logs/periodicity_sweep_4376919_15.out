Sweep flags:  --use-alpha-anneal --use-two-critic --use-alpha-gate --use-int-norm

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2531
    train step  10000/20000 | loss=5.2421
    train step  15000/20000 | loss=5.2450
    train step  20000/20000 | loss=5.2633
    train step   5000/20000 | loss=1.3849
    train step  10000/20000 | loss=1.3863
    train step  15000/20000 | loss=1.3776
    train step  20000/20000 | loss=1.3857

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.3167
    train step  10000/20000 | loss=5.3292
    train step  15000/20000 | loss=5.3267
    train step  20000/20000 | loss=5.3227
    train step   5000/20000 | loss=1.0414
    train step  10000/20000 | loss=0.7971
    train step  15000/20000 | loss=0.6622
    train step  20000/20000 | loss=0.5221

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2284
    train step  10000/20000 | loss=5.2341
    train step  15000/20000 | loss=5.2204
    train step  20000/20000 | loss=5.2224
    train step   5000/20000 | loss=1.3330
    train step  10000/20000 | loss=1.3047
    train step  15000/20000 | loss=1.2031
    train step  20000/20000 | loss=1.0502

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2266
    train step  10000/20000 | loss=5.2300
    train step  15000/20000 | loss=5.2292
    train step  20000/20000 | loss=5.2133
    train step   5000/20000 | loss=1.3856
    train step  10000/20000 | loss=1.3865
    train step  15000/20000 | loss=1.3829
    train step  20000/20000 | loss=1.3811

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2520
    train step  10000/20000 | loss=5.2562
    train step  15000/20000 | loss=5.2346
    train step  20000/20000 | loss=5.2399
    train step   5000/20000 | loss=1.3053
    train step  10000/20000 | loss=1.2631
    train step  15000/20000 | loss=1.1946
    train step  20000/20000 | loss=1.1552

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
Study complete. Outputs in: /home/js6726/invariance_aware/periodicity_study/outputs_sweep/comb_15
