Sweep flags:  --use-alpha-gate

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2649
    train step  10000/20000 | loss=5.2587
    train step  15000/20000 | loss=5.2492
    train step  20000/20000 | loss=5.2539
    train step   5000/20000 | loss=1.3835
    train step  10000/20000 | loss=1.3842
    train step  15000/20000 | loss=1.3837
    train step  20000/20000 | loss=1.3798

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.3232
    train step  10000/20000 | loss=5.3016
    train step  15000/20000 | loss=5.3170
    train step  20000/20000 | loss=5.3193
    train step   5000/20000 | loss=1.0590
    train step  10000/20000 | loss=0.8174
    train step  15000/20000 | loss=0.6358
    train step  20000/20000 | loss=0.5567

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2349
    train step  10000/20000 | loss=5.2303
    train step  15000/20000 | loss=5.2132
    train step  20000/20000 | loss=5.2245
    train step   5000/20000 | loss=1.3379
    train step  10000/20000 | loss=1.2918
    train step  15000/20000 | loss=1.1472
    train step  20000/20000 | loss=1.0799

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2236
    train step  10000/20000 | loss=5.2173
    train step  15000/20000 | loss=5.2241
    train step  20000/20000 | loss=5.2165
    train step   5000/20000 | loss=1.3853
    train step  10000/20000 | loss=1.3840
    train step  15000/20000 | loss=1.3798
    train step  20000/20000 | loss=1.3907

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2529
    train step  10000/20000 | loss=5.2544
    train step  15000/20000 | loss=5.2349
    train step  20000/20000 | loss=5.2354
    train step   5000/20000 | loss=1.3268
    train step  10000/20000 | loss=1.2588
    train step  15000/20000 | loss=1.2021
    train step  20000/20000 | loss=1.1550

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
Study complete. Outputs in: /home/js6726/invariance_aware/periodicity_study/outputs_sweep/comb_4
