Sweep flags:  --use-two-critic --use-alpha-gate

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2409
    train step  10000/20000 | loss=5.2561
    train step  15000/20000 | loss=5.2487
    train step  20000/20000 | loss=5.2411
    train step   5000/20000 | loss=1.3889
    train step  10000/20000 | loss=1.3875
    train step  15000/20000 | loss=1.3835
    train step  20000/20000 | loss=1.3938

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.3018
    train step  10000/20000 | loss=5.2898
    train step  15000/20000 | loss=5.3265
    train step  20000/20000 | loss=5.3038
    train step   5000/20000 | loss=1.0346
    train step  10000/20000 | loss=0.7995
    train step  15000/20000 | loss=0.6342
    train step  20000/20000 | loss=0.5483

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2233
    train step  10000/20000 | loss=5.2194
    train step  15000/20000 | loss=5.2297
    train step  20000/20000 | loss=5.2234
    train step   5000/20000 | loss=1.3303
    train step  10000/20000 | loss=1.2382
    train step  15000/20000 | loss=1.1768
    train step  20000/20000 | loss=1.0929

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2299
    train step  10000/20000 | loss=5.2253
    train step  15000/20000 | loss=5.2155
    train step  20000/20000 | loss=5.2121
    train step   5000/20000 | loss=1.3849
    train step  10000/20000 | loss=1.3887
    train step  15000/20000 | loss=1.3799
    train step  20000/20000 | loss=1.3773

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2470
    train step  10000/20000 | loss=5.2360
    train step  15000/20000 | loss=5.2307
    train step  20000/20000 | loss=5.2270
    train step   5000/20000 | loss=1.3335
    train step  10000/20000 | loss=1.2658
    train step  15000/20000 | loss=1.2019
    train step  20000/20000 | loss=1.1479

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
Study complete. Outputs in: /home/js6726/invariance_aware/periodicity_study/outputs_sweep/comb_6
