Sweep flags:  --use-alpha-anneal --use-int-norm

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (periodicity) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2343
    train step  10000/20000 | loss=5.2524
    train step  15000/20000 | loss=5.2321
    train step  20000/20000 | loss=5.2522
    train step   5000/20000 | loss=1.3838
    train step  10000/20000 | loss=1.3884
    train step  15000/20000 | loss=1.3846
    train step  20000/20000 | loss=1.3850

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (slippery) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.3110
    train step  10000/20000 | loss=5.3202
    train step  15000/20000 | loss=5.3201
    train step  20000/20000 | loss=5.2992
    train step   5000/20000 | loss=1.0333
    train step  10000/20000 | loss=0.8486
    train step  15000/20000 | loss=0.6346
    train step  20000/20000 | loss=0.6028

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (teacup) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2390
    train step  10000/20000 | loss=5.2113
    train step  15000/20000 | loss=5.2208
    train step  20000/20000 | loss=5.2190
    train step   5000/20000 | loss=1.3478
    train step  10000/20000 | loss=1.2746
    train step  15000/20000 | loss=1.2424
    train step  20000/20000 | loss=1.1896

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Periodicity (Large) (periodicity_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.2394
    train step  10000/20000 | loss=5.2244
    train step  15000/20000 | loss=5.2226
    train step  20000/20000 | loss=5.2198
    train step   5000/20000 | loss=1.3816
    train step  10000/20000 | loss=1.3825
    train step  15000/20000 | loss=1.3848
    train step  20000/20000 | loss=1.3755

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Delay Action Queue (Large) (slippery_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
    train step   5000/20000 | loss=5.3029
    train step  10000/20000 | loss=5.2587
    train step  15000/20000 | loss=5.2699
    train step  20000/20000 | loss=5.2279
    train step   5000/20000 | loss=1.3198
    train step  10000/20000 | loss=1.2419
    train step  15000/20000 | loss=1.2127
    train step  20000/20000 | loss=1.1769

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_rep...
  Training PPO for coord_only_goal_rep...
  Training PPO for coord_plus_nuisance_rep...
  Training PPO for coord_plus_nuisance_goal_rep...
  Training PPO for crtr_learned_rep...
  Training PPO for crtr_learned_goal_rep...
  Training PPO for idm_learned_rep...
  Training PPO for idm_learned_goal_rep...
Stage 4: Online joint representations + bonus + PPO (rep)

=== Teacup Maze (Large) (teacup_large) ===
Stage 1: Representation invariance metrics
Stage 2: Elliptical bonus metrics + heatmaps
Stage 3: PPO action distribution invariance (bonus-only)
  Training PPO for coord_only_raw...
  Training PPO for coord_only_goal_raw...
  Training PPO for coord_plus_nuisance_raw...
  Training PPO for coord_plus_nuisance_goal_raw...
  Training PPO for crtr_learned_raw...
  Training PPO for crtr_learned_goal_raw...
  Training PPO for idm_learned_raw...
  Training PPO for idm_learned_goal_raw...
Stage 4: Online joint representations + bonus + PPO (raw)
Study complete. Outputs in: /home/js6726/invariance_aware/periodicity_study/outputs_sweep/comb_9
